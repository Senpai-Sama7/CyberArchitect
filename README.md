# CyberArchitect

## The Ultimate Website Replication Suite â€” Professionalâ€‘Grade Edition

[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen)](#)
[![License](https://img.shields.io/badge/License-MIT-yellow)](#)
[![Powered by Puppeteer](https://img.shields.io/badge/powered%20by-Puppeteer-blue)](#)

---
**CyberArchitect** is a robust, highâ€‘performance Node.js toolkit for comprehensive website replication, archiving, and analysis. Built with professional workflows in mind, it emphasizes resilience, efficiency, and respectful crawling of modern web applications.
---

### âœ¨ Key Features

#### ğŸš€ Core Replication & Performance

* **True streaming pipeline** â€“ assets stream directly from the network to disk, minimizing RAM.
* **Optional Brotli compression** â€“ shrink HTML, CSS, and JS to save disk space and bandwidth.
* **Optimized image & SVG handling** â€“ onâ€‘theâ€‘fly AVIF/WebP conversion and SVG minification.
* **CSS & HTML minification** â€“ lean, productionâ€‘ready output.

#### ğŸ›¡ï¸ Network Resilience & Control

* **Perâ€‘domain concurrency limits** â€“ avoid hammering any single host.
* **Perâ€‘domain circuit breakers** â€“ isolate failures; a downed CDN wonâ€™t block the main site.
* **Exponential backâ€‘off with jitter** â€“ intelligent retries for transient errors.
* **Lazy 404 caching** â€“ skip repeatedly missing assets.

#### ğŸ§  Advanced Crawling & Discovery

* **`sitemap.xml` discovery** â€“ automatically harvest hidden URLs.
* **Intelligent SPA crawling** â€“ simulates user interactions and detects cycles.
* **Robots.txt compliance** â€“ ethical, specâ€‘compliant crawling.

#### ğŸ”„ Integrity & Auditing

* **Incremental replication (`--incremental`)** â€“ update only what changed using ETag & Lastâ€‘Modified.
* **Integrity manifest** â€“ SHAâ€‘256 hashes for every asset.
* **`verify` command** â€“ reâ€‘hash local files and validate against the manifest.

#### ğŸ› ï¸ Developer Experience

* **Professional CLI** powered by *yargs* (`replicate`, `verify`, `--help`).
* **Structured logging** with *pino*.
* **Memory monitoring** with optional GC triggers (`--expose-gc`).
* **Crossâ€‘platform compatibility** â€“ automatic `fetch` polyfill for NodeÂ 16.

---

### ğŸ“¦ Installation

#### Prerequisites

* Node.js **â‰¥Â 18**
* npm

```bash
# Clone the repository
git clone https://github.com/Senpai-sama7/CyberArchitect.git
cd CyberArchitect

# Install dependencies (Chromium is downloaded automatically)
npm install
```

---

### ğŸš€ Usage

#### Replicate a website

```bash
node replicator.js replicate <url> [outputDir] [options]
```

Examples:

```bash
# Basic replication
node replicator.js replicate https://example.com ./myâ€‘replica

# Deep crawl + responsive screenshots
node replicator.js replicate https://myspa.com ./spaâ€‘archive --depth 3 --responsive

# Incremental update with Brotli
node replicator.js replicate https://myblog.com ./blogâ€‘update --incremental --brotli

# Fineâ€‘tuned concurrency
node replicator.js replicate https://complexâ€‘site.com ./complex --pageConcurrency 2 --baseAssetConcurrency 15 --domainAssetConcurrency 5
```

#### Verify a replica

```bash
node replicator.js verify <outputDir>
# e.g.
node replicator.js verify ./myâ€‘replica
```

#### Get help

```bash
node replicator.js --help
node replicator.js replicate --help
```

---

### âš™ï¸ Configuration Options (CLI flags)

| Option                     | Type    | Default             | Description                                           |
| -------------------------- | ------- | ------------------- | ----------------------------------------------------- |
| `<url>`                    | string  | **required**        | Target URL to replicate                               |
| `[outputDir]`              | string  | `./replicatedâ€‘site` | Output directory                                      |
| `--depth`                  | number  | `2`                 | Max crawl depth for SPA crawling (0Â = main page only) |
| `--pageConcurrency`        | number  | `4`                 | Concurrent browser pages                              |
| `--baseAssetConcurrency`   | number  | `10`                | Concurrent asset downloads on main domain             |
| `--domainAssetConcurrency` | number  | `3`                 | Concurrent asset downloads per external domain        |
| `--incremental`            | boolean | `false`             | Incremental updates using ETag/Lastâ€‘Modified          |
| `--brotli`                 | boolean | `false`             | Enable Brotli compression                             |
| `--crawlSPA`               | boolean | `true`              | Enable SPA crawling                                   |
| `--respectRobotsTxt`       | boolean | `true`              | Obey robots.txt                                       |
| ...                        |         |                     | *(see `--help` for the full list)*                    |

---

### ğŸ’¡ Advanced Topics

#### Incremental replication

When `--incremental` is enabled CyberArchitect loads the existing `manifest.json`, reâ€‘validates assets with `Ifâ€‘Noneâ€‘Match`/`Ifâ€‘Modifiedâ€‘Since`, and downloads only what changed.

#### Domainâ€‘based concurrency

`--baseAssetConcurrency` and `--domainAssetConcurrency` ensure fair use of network resources by throttling each host independently.

#### Brotli compression

With `--brotli`, HTML/CSS/JS are stored as `.br` files. Configure your web server to serve these when the client advertises `Acceptâ€‘Encoding: br`.

---

### ğŸ¤ Contributing

Pull requests and feature ideas are welcomeâ€”check the issue tracker first to avoid duplication.

### ğŸ“„ License

CyberArchitect is released under the **MIT License**.
